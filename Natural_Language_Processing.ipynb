{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPEvZcjikbCIcf+m6P6ta2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/levitannin/Madlib-Workshop/blob/main/Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUI7NVLeuJck"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "Welcome, you made it!  This is a supplimentary notebook for the [Madlib Workshop](https://github.com/levitannin/Madlib-Workshop/blob/main/MadLib_Workshop.ipynb)!  This notebook works just fine as a standalone too.  Feel free to use this as a baseline for future endeavours!\n",
        "\n",
        "In this notebook we are focused with the basics of Natural Language Processing and the **Natrual Language Toolkit** (NLTK) library.  The best place to learn about this tool is the NLTK Documentation! [Click Me to Learn!](https://www.nltk.org/)\n",
        "\n",
        "Remember, we are working with Python 3 at this point so avoid looking into Python 2 versions of this library where you can.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqg4FMHlzNaa"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKHyZ5H_GVA0"
      },
      "source": [
        "import nltk\n",
        "# You may need this to download all of the nltk dependancies or extras!\n",
        "nltk.download(\"book\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnNCg55EuFD2"
      },
      "source": [
        "\"\"\"\n",
        "Let's start with Tokenizing!\n",
        "\n",
        "Tokenizing can be broken down into:\n",
        "  Word Tokenizing == Break down Words\n",
        "  Sentence Tokenizing == Break down Sentences\n",
        "\n",
        "Other terms:\n",
        "  Corpora == a body of text; see corpus\n",
        "  Lexicon == dictionary; words and their meanings\n",
        "\"\"\"\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "example_text = \"Hello Mr. Stranger, how are you doing today?  The weather is great and python is awesome.  Let's meet later for tea.  We can discuss why the sky is pinkish blue, and how that tells us not to eat cardboard.\"\n",
        "\n",
        "print(sent_tokenize(example_text))\n",
        "print(word_tokenize(example_text))\n",
        "# NOTE: word tokenize will treat a punctuation as a 'word' by default.\n",
        "\n",
        "# Below will print each word outide of the list.\n",
        "\n",
        "for i in word_tokenize(example_text):\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2h7vbx4uJAe"
      },
      "source": [
        "\"\"\"\n",
        "Okay so now we understand tokenization, let's build on that!\n",
        "\n",
        "Now that we have an example of text, let's figure out what the 'stop words' are.\n",
        "We are expecting these words to be from the English language for now.\n",
        "\"\"\"\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "print(stop_words)\n",
        "\n",
        "words = word_tokenize(example_text)\n",
        "filtered_sentence = [w for w in words if not w in stop_words]\n",
        "\n",
        "print(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VU-2mY700jf"
      },
      "source": [
        "\"\"\"\n",
        "We are on a roll!  Now let's dig into stemming and what this means for us.\n",
        "What is a stem?  Well, think of the endings of words like 'ing', 'ed', etc.\n",
        "We want the root (or stem, get it?) of a word instead of the extra fluff (leaves?)\n",
        "\n",
        "To get those stems we need to do a little processing.  Why?\n",
        "  If two words would be 'the same' but are in different tenses, this can cause\n",
        "  clutter.  To avoid that, we use stemming.\n",
        "\"\"\"\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for w in words:\n",
        "  print(ps.stem(w))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYQOV8A79lra"
      },
      "source": [
        "\"\"\"\n",
        "Now we are getting into the fun stuff.  If you're following along with the\n",
        "  Madlib Workshop, this is the part we'll really be using to organize the text.\n",
        "\n",
        "At this stage we are focused on identifying parts of speech (PoS)\n",
        "  and on the corpus or works within nltk.\n",
        "\"\"\"\n",
        "\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from nltk.corpus import gutenberg, state_union\n",
        "from nltk import pos_tag\n",
        "\n",
        "#   Speech Tagging -- Identifying the different parts of speech.\n",
        "#-----------------------------------------------------------------------------\n",
        "#   Identify what items the imported corpus may have.\n",
        "\n",
        "# Gutenberg is an excellent resource, highly recommend checking out the website.\n",
        "print(\"\\nText available from Gutenberg: \\n\")\n",
        "print(gutenberg.fileids())\n",
        "print(\"\\nText available from State of the Union: \\n\")\n",
        "print(state_union.fileids())\n",
        "\n",
        "#   Useful tools we can use on the imported text.\n",
        "#-----------------------------------------------------------------------------\n",
        "#   The following will create a table to identify:\n",
        "#       Average word length\n",
        "#       Average sentence length\n",
        "#       Frequency of a vocab word appearing\n",
        "#   For each text in the Gutenberg corpus\n",
        "print(\"Ave Word Len \\t Ave Sent Len \\t Vocab Occurance \\t Title \")\n",
        "for fileid in gutenberg.fileids():\n",
        "    num_chars = len(gutenberg.raw(fileid))\n",
        "    num_words = len(gutenberg.words(fileid))\n",
        "    num_sents = len(gutenberg.sents(fileid))\n",
        "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
        "    \n",
        "    print(round(num_chars / num_words), \"\\t\\t\", round(num_words / num_sents),\n",
        "          \"\\t\\t\", round(num_words / num_vocab), \"\\t\\t\", fileid)\n",
        "\n",
        "# Now let's just focus on one of the text.\n",
        "paradise_sent = gutenberg.sents(\"milton-paradise.txt\")\n",
        "print(len(paradise_sent))\n",
        "\n",
        "#   Print a sentence in the corpus after breaking into sents(ences)\n",
        "print(paradise_sent[1313])\n",
        "\n",
        "#   Identify the longest sentence in the chosen corpus.\n",
        "longest_sent = max(len(s) for s in paradise_sent)\n",
        "print(s for s in paradise_sent if len (s) == longest_sent)\n",
        "\n",
        "#   Can find other text sources at: https://www.nltk.org/book/ch02.html\n",
        "#   Raw gives the content of the file without any linguistic processing.\n",
        "train_text = state_union.raw(\"1953-Eisenhower.txt\")\n",
        "sample_text = state_union.raw(\"1959-Eisenhower.txt\")\n",
        "#   If you want to re-train the Punkt for your purposes\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "token = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "'''\n",
        "POS tag list:\n",
        "    CC      coordinating conjunction\n",
        "    CD      cardinal digit\n",
        "    DT      determiner\n",
        "    EX      existential there (like: \"there is\" ... think of it like \"there exists\")\n",
        "    FW      foreign word\n",
        "    IN      preposition/subordinating conjunction\n",
        "    JJ      adjective 'big'\n",
        "    JJR     adjective, comparative 'bigger'\n",
        "    JJS     adjective, superlative 'biggest'\n",
        "    LS      list marker 1)\n",
        "    MD      modal could, will\n",
        "    NN      noun, singular 'desk'\n",
        "    NNS     noun plural 'desks'\n",
        "    NNP     proper noun, singular 'Harrison'\n",
        "    NNPS    proper noun, plural 'Americans'\n",
        "    PDT     predeterminer 'all the kids'\n",
        "    POS     possessive ending parent's\n",
        "    PRP     personal pronoun I, he, she\n",
        "    PRP$    possessive pronoun my, his, hers\n",
        "    RB      adverb very, silently,\n",
        "    RBR     adverb, comparative better\n",
        "    RBS     adverb, superlative best\n",
        "    RP      particle give up\n",
        "    TO      to go 'to' the store.\n",
        "    UH      interjection errrrrrrrm\n",
        "    VB      verb, base form take\n",
        "    VBD     verb, past tense took\n",
        "    VBG     verb, gerund/present participle taking\n",
        "    VBN     verb, past participle taken\n",
        "    VBP     verb, sing. present, non-3d take\n",
        "    VBZ     verb, 3rd person sing. present takes\n",
        "    WDT     wh-determiner which\n",
        "    WP      wh-pronoun who, what\n",
        "    WP$     possessive wh-pronoun whose\n",
        "    WRB     wh-abverb where, when\n",
        "'''\n",
        "try:\n",
        "    for i in token:#    You can specify here if you want to start at a certain level of the chunk.  IE [5:]\n",
        "        words = word_tokenize(i)\n",
        "        tag = pos_tag(words)\n",
        "        print(tag)\n",
        "        \n",
        "except Exception as e:\n",
        "    print(str(e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bc3BhlhCQMM"
      },
      "source": [
        "\"\"\"\n",
        "Now that we understand PoS, let's go into chunking.\n",
        "This will let us identify or group words based on parts of speech.\n",
        "\"\"\"\n",
        "\n",
        "from nltk import ne_chunk, RegexpParser\n",
        "\n",
        "try:\n",
        "    for i in token:#    You can specify here if you want to start at a certain level of the chunk.  IE [5:]\n",
        "        words = word_tokenize(i)\n",
        "        tag = pos_tag(words)\n",
        "        \n",
        "        #   To find all versions of a POS use regular expressions to identify it.\n",
        "        #   Example here are Adverbs (RB, RBR, RBS)\n",
        "        #   . == any character other than new line\n",
        "        #   ? == 0 or 1\n",
        "        #   * == 0 or MORE\n",
        "        #   | == or\n",
        "        chunkGram = r\"\"\" Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?} \"\"\"\n",
        "        chunkParser = RegexpParser(chunkGram)\n",
        "        chunked = chunkParser.parse(tag)\n",
        "        #   Will generate a pop-up box with the chucks drawn out in a chart.\n",
        "        #     Or would in an IDE rather than Google Colab, which doesn't have\n",
        "        #       access to your computer's display.\n",
        "        chunked.draw()\n",
        "        \n",
        "except Exception as e:\n",
        "    print(str(e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Ho8onnCpil"
      },
      "source": [
        "try:\n",
        "    for i in token[6:]:#    You can specify here if you want to start at a certain level of the chunk.  IE [5:]\n",
        "        words = word_tokenize(i)\n",
        "        tag = pos_tag(words)\n",
        "        chunkGram = r\"\"\" Chunk: {<.*>+} \n",
        "                                }<VB.?|IN|DT>+{\"\"\"\n",
        "        chunkParser = RegexpParser(chunkGram)\n",
        "        chunked = chunkParser.parse(tag)\n",
        "        #   Will generate a pop-up box with the chucks drawn out in a chart.\n",
        "        #     Or would in an IDE rather than Google Colab, which doesn't have\n",
        "        #       access to your computer's display.\n",
        "        chunked.draw()\n",
        "        \n",
        "except Exception as e:\n",
        "    print(str(e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqqYkNGETZWi"
      },
      "source": [
        "This is the end!  You did it :)\n",
        "\n",
        "If you're looking for more resources, check out these other colab notebooks created by others with an NLP focus:\n",
        "\n",
        "\n",
        "*   https://colab.research.google.com/github/gal-a/blog/blob/master/docs/notebooks/nlp/nltk_preprocess.ipynb\n",
        "*   https://colab.research.google.com/github/mhuckvale/pals0039/blob/master/Tutorial_NLTK.ipynb#scrollTo=02RtRYj_p0Xb \n",
        "\n"
      ]
    }
  ]
}